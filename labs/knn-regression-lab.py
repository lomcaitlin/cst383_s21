import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsRegressordf = pd.read_csv('https://raw.githubusercontent.com/grbruns/cst383/master/College.csv', index_col=0)df.info()# predict tuition of college based on # of students in top 10% and # of undergrads# 3 : 2d numpy array from 'Top10perc' and 'F.Undergrad'X = df[['Top10perc', 'F.Undergrad']].values# 4 : 1d numpy array from 'Outstate'y = df['Outstate'].values# 5 : split data with 30% data in training setX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) # does random state always need to be 42?    # 6 : scale data valuesscaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)# 7 : build kNN regressor and trainknn = KNeighborsRegressor()knn.fit(X_train, y_train)# 8 : make predictionspredictions = knn.predict(X_test)# 9 : compare first 10 predictions to first 10 actual valuespredictions[:10]y_test[:10]# 10 : 2 vars needed for mse?## predictions & y_test# 11 : compute mseMSE = ((predictions - y_test)**2).mean()MSE# 12 : MSE value if replace prediction with y_train.mean() (((y_train.mean()) - y_test)**2).mean()#13 : repeat 9-13 with k=7knn = KNeighborsRegressor(n_neighbors=7)knn.fit(X_train, y_train)predictions = knn.predict(X_test)predictions[:10]y_test[:10]MSE = ((predictions - y_test)**2).mean()print(MSE)(((y_train.mean()) - y_test)**2).mean()## yes it improves the result# 14 : 